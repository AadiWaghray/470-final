@article{akhandFacialEmotionRecognition2021,
  title = {Facial {{Emotion Recognition Using Transfer Learning}} in the {{Deep CNN}}},
  author = {Akhand, M. a. H. and Roy, Shuvendu and Siddique, Nazmul and Kamal, Md Abdus Samad and Shimamura, Tetsuya},
  year = {2021},
  month = jan,
  journal = {Electronics},
  volume = {10},
  number = {9},
  pages = {1036},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics10091036},
  urldate = {2024-04-14},
  abstract = {Human facial emotion recognition (FER) has attracted the attention of the research community for its promising applications. Mapping different facial expressions to the respective emotional states are the main task in FER. The classical FER consists of two major steps: feature extraction and emotion recognition. Currently, the Deep Neural Networks, especially the Convolutional Neural Network (CNN), is widely used in FER by virtue of its inherent feature extraction mechanism from images. Several works have been reported on CNN with only a few layers to resolve FER problems. However, standard shallow CNNs with straightforward learning schemes have limited feature extraction capability to capture emotion information from high-resolution images. A notable drawback of the most existing methods is that they consider only the frontal images (i.e., ignore profile views for convenience), although the profile views taken from different angles are important for a practical FER system. For developing a highly accurate FER system, this study proposes a very Deep CNN (DCNN) modeling through Transfer Learning (TL) technique where a pre-trained DCNN model is adopted by replacing its dense upper layer(s) compatible with FER, and the model is fine-tuned with facial emotion data. A novel pipeline strategy is introduced, where the training of the dense layer(s) is followed by tuning each of the pre-trained DCNN blocks successively that has led to gradual improvement of the accuracy of FER to a higher level. The proposed FER system is verified on eight different pre-trained DCNN models (VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-152, Inception-v3 and DenseNet-161) and well-known KDEF and JAFFE facial image datasets. FER is very challenging even for frontal views alone. FER on the KDEF dataset poses further challenges due to the diversity of images with different profile views together with frontal views. The proposed method achieved remarkable accuracy on both datasets with pre-trained models. On a 10-fold cross-validation way, the best achieved FER accuracies with DenseNet-161 on test sets of KDEF and JAFFE are 96.51\% and 99.52\%, respectively. The evaluation results reveal the superiority of the proposed FER system over the existing ones regarding emotion detection accuracy. Moreover, the achieved performance on the KDEF dataset with profile views is promising as it clearly demonstrates the required proficiency for real-life applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural network (CNN),deep CNN,emotion recognition,transfer learning},
  file = {/Users/aadiwaghray/Zotero/storage/2GNXFZVP/Akhand et al. - 2021 - Facial Emotion Recognition Using Transfer Learning.pdf}
}

@inproceedings{alyFacialEmotionRecognition2019,
  title = {Facial {{Emotion Recognition}} with {{Varying Poses}} and/or {{Partial Occlusion Using Multi-stage Progressive Transfer Learning}}},
  booktitle = {Image {{Analysis}}},
  author = {Aly, Sherin F. and Abbott, A. Lynn},
  editor = {Felsberg, Michael and Forss{\'e}n, Per-Erik and Sintorn, Ida-Maria and Unger, Jonas},
  year = {2019},
  pages = {101--112},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-20205-7_9},
  abstract = {This paper describes the use of multi-stage Progressive Transfer Learning (MSPTL) to improve the performance of automated Facial Emotion Recognition (FER). Our proposed FER solution is designed to work with 2D images, and is able to classify facial emotions with high accuracy in 6 basic categories (happiness, sadness, fear, anger, surprise, and disgust) for both frontal and (more challenging) non-frontal poses. We perform supervised fine-tuning on an AlexNet deep convolutional neural network in a three-stage process, using three FER datasets in succession. The first two training stages are based on FER datasets containing frontal images only. The final training stage uses a third FER dataset that includes non-frontal poses in images that are relatively low in resolution and/or with partial occlusion. Experimental results demonstrate that our proposed MSPTL approach outperforms typical TL and other PTL systems for FER in both frontal and non-frontal face poses. These results are demonstrated using two different testing datasets (VT-KFER and 300W), which corroborates the generality of the proposed solution and its robustness for handling a wide range of varying poses, occlusion, and expression intensities.},
  isbn = {978-3-030-20205-7},
  langid = {english},
  file = {/Users/aadiwaghray/Zotero/storage/KILP3H89/Aly and Abbott - 2019 - Facial Emotion Recognition with Varying Poses and.pdf}
}

@article{alzubaidiReviewDeepLearning2021,
  title = {Review of Deep Learning: Concepts, {{CNN}} Architectures, Challenges, Applications, Future Directions},
  shorttitle = {Review of Deep Learning},
  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and {Al-Dujaili}, Ayad and Duan, Ye and {Al-Shamma}, Omran and Santamar{\'i}a, J. and Fadhel, Mohammed A. and {Al-Amidie}, Muthana and Farhan, Laith},
  year = {2021},
  month = mar,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {53},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00444-8},
  urldate = {2024-04-14},
  abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
  keywords = {Convolution neural network (CNN),Deep learning,Deep learning applications,Deep neural network architectures,FPGA,GPU,Image classification,Machine learning,Medical image analysis,Supervised learning,Transfer learning},
  file = {/Users/aadiwaghray/Zotero/storage/5WPILS79/Alzubaidi et al. - 2021 - Review of deep learning concepts, CNN architectur.pdf;/Users/aadiwaghray/Zotero/storage/A5GUVID4/s40537-021-00444-8.html}
}

@article{bhandariCanEdgesHelp2021,
  title = {Can Edges Help Convolution Neural Networks in Emotion Recognition?},
  author = {Bhandari, Arkaprabha and Pal, Nikhil R.},
  year = {2021},
  month = apr,
  journal = {Neurocomputing},
  volume = {433},
  pages = {162--168},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.12.092},
  urldate = {2024-04-15},
  abstract = {Facial emotion recognition has gained importance for its applications in diverse areas. Facial expressions of a subject, when experiencing the same emotion, have wider variations. On the other hand, different subjects experiencing the same emotion may exhibit different facial features. All these make facial emotion recognition challenging. The ability of convolutional neural network (CNN) has been exploited to analyze visual imagery for different applications. It has also been used in developing automatic facial emotion recognition systems. Our objective in this study is to check if an explicit use of edges can help emotion recognition from images using CNN. Edges in an image represent discriminatory information and hence their explicit use is likely to help the training of CNNs and improve emotion recognition. Keeping this in mind we propose a two-tower CNN architecture to classify images into seven basic classes of emotion including the neutral expression. The proposed CNN has an additional tower, called edge-tower, which is simpler in architecture compared to the other tower and it uses edge images as inputs. Our experiments on two benchmark datasets demonstrate that the explicit use of edge information improves the classifier performance.},
  keywords = {Convolutional neural network,Edge images,Edge-tower,Emotion recognition,Two-tower CNN},
  file = {/Users/aadiwaghray/Zotero/storage/X8AXBU85/Bhandari and Pal - 2021 - Can edges help convolution neural networks in emot.pdf;/Users/aadiwaghray/Zotero/storage/HPRV6AWW/S092523122032004X.html}
}

@article{canalSurveyFacialEmotion2022,
  title = {A Survey on Facial Emotion Recognition Techniques: {{A}} State-of-the-Art Literature Review},
  shorttitle = {A Survey on Facial Emotion Recognition Techniques},
  author = {Canal, Felipe Zago and M{\"u}ller, Tobias Rossi and Matias, Jhennifer Cristine and Scotton, Gustavo Gino and {de Sa Junior}, Antonio Reis and Pozzebon, Eliane and Sobieranski, Antonio Carlos},
  year = {2022},
  month = jan,
  journal = {Information Sciences},
  volume = {582},
  pages = {593--617},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2021.10.005},
  urldate = {2024-04-14},
  abstract = {In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of neural networks. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to neural networks counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments.},
  keywords = {Emotion Recognition,Facial emotion recognition,Pattern recognition,Systematic literature review},
  file = {/Users/aadiwaghray/Zotero/storage/M4K38YBR/Canal et al. - 2022 - A survey on facial emotion recognition techniques.pdf;/Users/aadiwaghray/Zotero/storage/L8CMXYZC/S0020025521010136.html}
}

@misc{dachapallyFacialEmotionDetection2017,
  title = {Facial {{Emotion Detection Using Convolutional Neural Networks}} and {{Representational Autoencoder Units}}},
  author = {Dachapally, Prudhvi Raj},
  year = {2017},
  month = jun,
  number = {arXiv:1706.01509},
  eprint = {1706.01509},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.01509},
  urldate = {2024-04-14},
  abstract = {Emotion being a subjective thing, leveraging knowledge and science behind labeled data and extracting the components that constitute it, has been a challenging problem in the industry for many years. With the evolution of deep learning in computer vision, emotion recognition has become a widely-tackled research problem. In this work, we propose two independent methods for this very task. The first method uses autoencoders to construct a unique representation of each emotion, while the second method is an 8-layer convolutional neural network (CNN). These methods were trained on the posed-emotion dataset (JAFFE), and to test their robustness, both the models were also tested on 100 random images from the Labeled Faces in the Wild (LFW) dataset, which consists of images that are candid than posed. The results show that with more fine-tuning and depth, our CNN model can outperform the state-of-the-art methods for emotion recognition. We also propose some exciting ideas for expanding the concept of representational autoencoders to improve their performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/aadiwaghray/Zotero/storage/2YNUEQC7/Dachapally - 2017 - Facial Emotion Detection Using Convolutional Neura.pdf;/Users/aadiwaghray/Zotero/storage/JSZU8TI2/1706.html}
}

@article{dalviSurveyAIBasedFacial2021,
  title = {A {{Survey}} of {{AI-Based Facial Emotion Recognition}}: {{Features}}, {{ML}} \& {{DL Techniques}}, {{Age-Wise Datasets}} and {{Future Directions}}},
  shorttitle = {A {{Survey}} of {{AI-Based Facial Emotion Recognition}}},
  author = {Dalvi, Chirag and Rathod, Manish and Patil, Shruti and Gite, Shilpa and Kotecha, Ketan},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {165806--165840},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3131733},
  urldate = {2024-04-14},
  abstract = {Facial expressions are mirrors of human thoughts and feelings. It provides a wealth of social cues to the viewer, including the focus of attention, intention, motivation, and emotion. It is regarded as a potent tool of silent communication. Analysis of these expressions gives a significantly more profound insight into human behavior. AI-based Facial Expression Recognition (FER) has become one of the crucial research topics in recent years, with applications in dynamic analysis, pattern recognition, interpersonal interaction, mental health monitoring, and many more. However, with the global push towards online platforms due to the Covid-19 pandemic, there has been a pressing need to innovate and offer a new FER analysis framework with the increasing visual data generated by videos and photographs.Furthermore, the emotion-wise facial expressions of kids, adults, and senior citizens vary, which must also be considered in the FER research. Lots of research work has been done in this area. However, it lacks a comprehensive overview of the literature that showcases the past work done and provides the aligned future directions. In this paper, the authors have provided a comprehensive evaluation of AI-based FER methodologies, including datasets, feature extraction techniques, algorithms, and the recent breakthroughs with their applications in facial expression identification. To the best of the author's knowledge, this is the only review paper stating all aspects of FER for various age brackets and would significantly impact the research community in the coming years.},
  keywords = {Emotion recognition,Face recognition,Facial emotion recognition (FER),facial expressions,feature extraction,machine learning,Market research,Psychology,Symbiosis,Videos},
  file = {/Users/aadiwaghray/Zotero/storage/ITIJA5NK/Dalvi et al. - 2021 - A Survey of AI-Based Facial Emotion Recognition F.pdf}
}

@article{diasCrossdatasetEmotionRecognition2022,
  title = {Cross-Dataset Emotion Recognition from Facial Expressions through Convolutional Neural Networks},
  author = {Dias, William and Andal{\'o}, Fernanda and Padilha, Rafael and Bertocco, Gabriel and Almeida, Waldir and Costa, Paula and Rocha, Anderson},
  year = {2022},
  month = jan,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {82},
  pages = {103395},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2021.103395},
  urldate = {2024-04-15},
  abstract = {The face is the window to the soul. This is what the 19th-century French doctor Duchenne de Boulogne thought. Using electric shocks to stimulate muscular contractions and induce bizarre-looking expressions, he wanted to understand how muscles produce facial expressions and reveal the most hidden human emotions. Two centuries later, this research field remains very active. We see automatic systems for recognizing emotion and facial expression being applied in medicine, security and surveillance systems, advertising and marketing, among others. However, there are still fundamental questions that scientists are trying to answer when analyzing a person's emotional state from their facial expressions. Is it possible to reliably infer someone's internal state based only on their facial muscles' movements? Is there a universal facial setting to express basic emotions such as anger, disgust, fear, happiness, sadness, and surprise? In this research, we seek to address some of these questions through convolutional neural networks. Unlike most studies in the prior art, we are particularly interested in examining whether characteristics learned from one group of people can be generalized to predict another's emotions successfully. In this sense, we adopt a cross-dataset evaluation protocol to assess the performance of the proposed methods. Our baseline is a custom-tailored model initially used in face recognition to categorize emotion. By applying data visualization techniques, we improve our baseline model, deriving two other methods. The first method aims to direct the network's attention to regions of the face considered important in the literature but ignored by the baseline model, using patches to hide random parts of the facial image so that the network can learn discriminative characteristics in different regions. The second method explores a loss function that generates data representations in high-dimensional spaces so that examples of the same emotion class are close and examples of different classes are distant. Finally, we investigate the complementarity between these two methods, proposing a late-fusion technique that combines their outputs through the multiplication of probabilities. We compare our results to an extensive list of works evaluated in the same adopted datasets. In all of them, when compared to works that followed an intra-dataset protocol, our methods present competitive numbers. Under a cross-dataset protocol, we achieve state-of-the-art results, outperforming even commercial off-the-shelf solutions from well-known tech companies.},
  keywords = {Cross-dataset evaluation,Deep learning,Emotion recognition,Facial analysis},
  file = {/Users/aadiwaghray/Zotero/storage/6NUXB8DF/Dias et al. - 2022 - Cross-dataset emotion recognition from facial expr.pdf;/Users/aadiwaghray/Zotero/storage/LJESTPR7/S1047320321002637.html}
}

@article{FacialEmotionRecognition2023,
  title = {Facial Emotion Recognition Methods, Datasets and Technologies: {{A}} Literature Survey},
  shorttitle = {Facial Emotion Recognition Methods, Datasets and Technologies},
  year = {2023},
  month = jan,
  journal = {Materials Today: Proceedings},
  volume = {80},
  pages = {2824--2828},
  publisher = {{Elsevier}},
  issn = {2214-7853},
  doi = {10.1016/j.matpr.2021.07.046},
  urldate = {2024-04-14},
  abstract = {Amidst of the technologies applied to analyze and recognize a facial emotion, there are still clustered challenges that needs to be addressed while bu\ldots},
  langid = {american},
  file = {/Users/aadiwaghray/Zotero/storage/Q7DDV4JI/2023 - Facial emotion recognition methods, datasets and t.pdf;/Users/aadiwaghray/Zotero/storage/Q83TY7XR/S2214785321048987.html}
}

@inproceedings{ghandiParticleSwarmOptimization2009,
  title = {Particle {{Swarm Optimization}} Algorithm for Facial Emotion Detection},
  booktitle = {2009 {{IEEE Symposium}} on {{Industrial Electronics}} \& {{Applications}}},
  author = {Ghandi, Bashir Mohammed and Nagarajan, R. and Desa, Hazry},
  year = {2009},
  month = oct,
  volume = {2},
  pages = {595--599},
  doi = {10.1109/ISIEA.2009.5356389},
  urldate = {2024-04-14},
  abstract = {Particle Swarm Optimization (PSO) algorithm has been applied and found to be efficient in many searching and optimization related applications. In this paper, we present a modified version of the algorithm that we successfully applied to facial emotion detection. Our approach is based on tracking the movements of facial action units (AUs) placed on the face of a subject and captured in video clips. We defined particles that form swarms such that they have a component around the neighborhood of each AU. Particles are allowed to move around the effectively n-dimensional search space in search of the emotion being expressed in each frame of a video clip (where n is the number of action units being tracked). We have implemented and tested the algorithm on video clips that contain three of the six basic emotions, namely happy, sad and surprise. Our results show the algorithm to have a promising success rate.},
  keywords = {emotion detection,Face detection,facial action units,facial emotions,facial expressions,Facial muscles,Gold,Humans,Industrial electronics,Intelligent robots,Mechatronics,particle swarm optimization,Particle swarm optimization,Power system simulation,PSO,Tracking},
  file = {/Users/aadiwaghray/Zotero/storage/2RQHFPP2/Ghandi et al. - 2009 - Particle Swarm Optimization algorithm for facial e.pdf}
}

@article{guptaDeepSelfattentionNetwork2020,
  title = {Deep Self-Attention Network for Facial Emotion Recognition},
  author = {Gupta, Arpita and Arunachalam, Subrahmanyam and Balakrishnan, Ramadoss},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Third {{International Conference}} on {{Computing}} and {{Network Communications}} ({{CoCoNet}}'19)},
  volume = {171},
  pages = {1527--1534},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.04.163},
  urldate = {2024-04-14},
  abstract = {Human emotion detection is one of the major problems in computer vision. Human emotions consist of several sub-emotions which are difficult to classify into a specific class. In this proposed work, we have tried to classify the emotions into 6 basic categories (happy, sad, disgust, fear, surprise, anger) and neutral human emotions. We have proposed deep-learning framework which consist of CNN, ResNet and attention block which gives visual perceptibility to the network. The proposed model has greater applicability in real life facial emotion detection. The proposed model has achieved satisfactory result and has shown effective results on FER dataset.},
  keywords = {Attention,Deep Learning,Facial Emotion Recognition,ResNet},
  file = {/Users/aadiwaghray/Zotero/storage/JHW4TS4X/Gupta et al. - 2020 - Deep self-attention network for facial emotion rec.pdf;/Users/aadiwaghray/Zotero/storage/W5Z3K2W9/S187705092031142X.html}
}

@article{josephFacialEmotionDetection2020,
  title = {Facial Emotion Detection Using Modified Eyemap\textendash Mouthmap Algorithm on an Enhanced Image and Classification with Tensorflow},
  author = {Joseph, Allen and Geetha, P.},
  year = {2020},
  month = mar,
  journal = {The Visual Computer},
  volume = {36},
  number = {3},
  pages = {529--539},
  issn = {1432-2315},
  doi = {10.1007/s00371-019-01628-3},
  urldate = {2024-04-14},
  abstract = {Detection of emotion using facial expression is a growing field of research. Facial expression detection is also helpful to identify the behavior of a person when a man interacts with the computer. In this work, facial expression recognition with respect to the changes in the facial geometry is proposed. First, the image is enhanced by means of discrete wavelet transform and fuzzy combination. Then, the facial geometry is found using the modified eyemap and mouthmap algorithm after finding the landmarks. Finally, the area and angle of the constructed triangles are found and classified using neural network with the help of tensorflow central processing unit version. Results show that the proposed algorithm is efficient in finding the facial emotion.},
  langid = {english},
  keywords = {Emotion,Eyemap,Facial expression,Facial geometry,Mouthmap},
  file = {/Users/aadiwaghray/Zotero/storage/LD29N99U/Joseph and Geetha - 2020 - Facial emotion detection using modified eyemap–mou.pdf}
}

@article{koBriefReviewFacial2018,
  title = {A {{Brief Review}} of {{Facial Emotion Recognition Based}} on {{Visual Information}}},
  author = {Ko, Byoung Chul},
  year = {2018},
  month = feb,
  journal = {Sensors},
  volume = {18},
  number = {2},
  pages = {401},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18020401},
  urldate = {2024-04-14},
  abstract = {Facial emotion recognition (FER) is an important topic in the fields of computer vision and artificial intelligence owing to its significant academic and commercial potential. Although FER can be conducted using multiple sensors, this review focuses on studies that exclusively use facial images, because visual expressions are one of the main information channels in interpersonal communication. This paper provides a brief review of researches in the field of FER conducted over the past decades. First, conventional FER approaches are described along with a summary of the representative categories of FER systems and their main algorithms. Deep-learning-based FER approaches using deep networks enabling ``end-to-end'' learning are then presented. This review also focuses on an up-to-date hybrid deep-learning approach combining a convolutional neural network (CNN) for the spatial features of an individual frame and long short-term memory (LSTM) for temporal features of consecutive frames. In the later part of this paper, a brief review of publicly available evaluation metrics is given, and a comparison with benchmark results, which are a standard for a quantitative comparison of FER researches, is described. This review can serve as a brief guidebook to newcomers in the field of FER, providing basic knowledge and a general understanding of the latest state-of-the-art studies, as well as to experienced researchers looking for productive directions for future work.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {conventional FER,convolutional neural networks,deep learning-based FER,facial action coding system,facial action unit,facial emotion recognition,long short term memory},
  file = {/Users/aadiwaghray/Zotero/storage/6TSXU3NZ/Ko - 2018 - A Brief Review of Facial Emotion Recognition Based.pdf}
}

@article{lakshmiFacialEmotionRecognition2021,
  title = {Facial Emotion Recognition Using Modified {{HOG}} and {{LBP}} Features with Deep Stacked Autoencoders},
  author = {Lakshmi, D. and Ponnusamy, R.},
  year = {2021},
  month = apr,
  journal = {Microprocessors and Microsystems},
  volume = {82},
  pages = {103834},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2021.103834},
  urldate = {2024-04-15},
  abstract = {Recognition of emotions using facial expression is an active research topic in the field of computer vision. In this paper, a novel feature descriptor proposed for facial expression recognition using modified Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) feature descriptor. Firstly, viola-Jones face detection used to detect the face region, Then, Butterworth high pass filter utilized to enhance the detected region to find the eye, nose and mouth region detection using Viola-Jones approach. Secondly, the proposed modified HOG and LBP feature descriptor are used to extract the features of the detected eye, nose and mouth regions. The extracted features of these three regions are concatenated and reduced its dimensionality using Deep Stacked AutoEncoders. Finally, multi-class Support Vector Machine is used for classification and recognition. Experimental results show that the proposed modified feature descriptors can effectively recognize emotions on CK+ dataset and JAFFE dataset.},
  keywords = {Deep stacked autoencoder,Facial emotion recognition,LBP,MHOG},
  file = {/Users/aadiwaghray/Zotero/storage/NDRE46VC/Lakshmi and Ponnusamy - 2021 - Facial emotion recognition using modified HOG and .pdf;/Users/aadiwaghray/Zotero/storage/YTMYJU3A/S0141933121000144.html}
}

@article{mehendaleFacialEmotionRecognition2020,
  title = {Facial Emotion Recognition Using Convolutional Neural Networks ({{FERC}})},
  author = {Mehendale, Ninad},
  year = {2020},
  month = feb,
  journal = {SN Applied Sciences},
  volume = {2},
  number = {3},
  pages = {446},
  issn = {2523-3971},
  doi = {10.1007/s42452-020-2234-1},
  urldate = {2024-04-14},
  abstract = {Facial expression for emotion detection has always been an easy task for humans, but achieving the same task with a computer algorithm is quite challenging. With the recent advancement in computer vision and machine learning, it is possible to detect emotions from images. In this paper, we propose a novel technique called facial emotion recognition using convolutional neural networks (FERC). The FERC is based on two-part convolutional neural network (CNN): The first-part removes the background from the picture, and the second part concentrates on the facial feature vector extraction. In FERC model, expressional vector (EV) is used to find the five different types of regular facial expression. Supervisory data were obtained from the stored database of 10,000 images (154 persons). It was possible to correctly highlight the emotion with 96\% accuracy, using~a EV of length 24 values. The two-level CNN works in series, and the last layer of perceptron adjusts the weights and exponent values with each iteration. FERC differs from generally followed strategies with single-level CNN, hence improving the accuracy. Furthermore, a novel background removal procedure~applied, before the generation of EV, avoids dealing with multiple problems that may occur (for example distance from the camera). FERC was extensively tested with more than 750K images using extended Cohn\textendash Kanade expression, Caltech faces, CMU and NIST datasets. We expect the FERC emotion detection to be useful in many applications such as predictive learning of students, lie detectors, etc.},
  langid = {english},
  keywords = {CNN,Emotion recognition,Facial expression},
  file = {/Users/aadiwaghray/Zotero/storage/4GHGBC4C/Mehendale - 2020 - Facial emotion recognition using convolutional neu.pdf}
}

@article{mehrabianInferenceAttitudesNonverbal1967,
  title = {Inference of Attitudes from Nonverbal Communication in Two Channels},
  author = {Mehrabian, Albert and Ferris, Susan R.},
  year = {1967},
  journal = {Journal of Consulting Psychology},
  volume = {31},
  number = {3},
  pages = {248--252},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {0095-8891},
  doi = {10.1037/h0024648},
  abstract = {3 DEGREES OF ATTITUDE (I.E., POSITIVE, NEUTRAL, AND NEGATIVE) IN FACIAL EXPRESSION WERE EACH COMBINED WITH 3 DEGREES OF ATTITUDE COMMUNICATED VOCALLY. THE VOCAL COMMUNICATIONS OF ATTITUDE WERE SUPERIMPOSED ON A NEUTRAL WORD. IN PREPARING THE 2-COMPONENT COMMUNICATIONS, THE COMPONENTS WERE SELECTED SO THAT THE DEGREE OF POSITIVE ATTITUDE COMMUNICATED FACIALLY WAS EQUIVALENT TO THAT COMMUNICATED VOCALLY-I.E., THE INDEPENDENT EFFECTS OF THE 2 COMPONENTS WERE COMPARABLE. IT WAS FOUND THAT ATTITUDES INFERRED FROM COMBINED FACIAL-VOCAL COMMUNICATIONS ARE A LINEAR FUNCTION OF THE ATTITUDES COMMUNICATED IN EACH COMPONENT, WITH THE FACIAL COMPONENT RECEIVING APPROXIMATELY 3/2 THE WEIGHT RECEIVED BY THE VOCAL COMPONENT. IMPLICATIONS OF THE FINDINGS FOR MORE GENERAL ATTITUDE-COMMUNICATION PROBLEMS ARE DISCUSSED. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Animal Vocalizations,Attitudes,Facial Expressions,Infant Vocalization,Social Perception}
}

@article{mehtaFacialEmotionRecognition2018,
  title = {Facial {{Emotion Recognition}}: {{A Survey}} and {{Real-World User Experiences}} in {{Mixed Reality}}},
  shorttitle = {Facial {{Emotion Recognition}}},
  author = {Mehta, Dhwani and Siddiqui, Mohammad Faridul Haque and Javaid, Ahmad Y.},
  year = {2018},
  month = feb,
  journal = {Sensors},
  volume = {18},
  number = {2},
  pages = {416},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18020416},
  urldate = {2024-04-14},
  abstract = {Extensive possibilities of applications have made emotion recognition ineluctable and challenging in the field of computer science. The use of non-verbal cues such as gestures, body movement, and facial expressions convey the feeling and the feedback to the user. This discipline of Human\textendash Computer Interaction places reliance on the algorithmic robustness and the sensitivity of the sensor to ameliorate the recognition. Sensors play a significant role in accurate detection by providing a very high-quality input, hence increasing the efficiency and the reliability of the system. Automatic recognition of human emotions would help in teaching social intelligence in the machines. This paper presents a brief study of the various approaches and the techniques of emotion recognition. The survey covers a succinct review of the databases that are considered as data sets for algorithms detecting the emotions by facial expressions. Later, mixed reality device Microsoft HoloLens (MHL) is introduced for observing emotion recognition in Augmented Reality (AR). A brief introduction of its sensors, their application in emotion recognition and some preliminary results of emotion recognition using MHL are presented. The paper then concludes by comparing results of emotion recognition by the MHL and a regular webcam.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {affect,augmented reality,emotion recognition,facial expressions,human\textendash computer interaction,intelligence,Microsoft HoloLens,sensors},
  file = {/Users/aadiwaghray/Zotero/storage/HEHTPKZZ/Mehta et al. - 2018 - Facial Emotion Recognition A Survey and Real-Worl.pdf}
}

@article{melloukFacialEmotionRecognition2020,
  title = {Facial Emotion Recognition Using Deep Learning: Review and Insights},
  shorttitle = {Facial Emotion Recognition Using Deep Learning},
  author = {Mellouk, Wafa and Handouzi, Wahida},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 17th {{International Conference}} on {{Mobile Systems}} and {{Pervasive Computing}} ({{MobiSPC}}),{{The}} 15th {{International Conference}} on {{Future Networks}} and {{Communications}} ({{FNC}}),{{The}} 10th {{International Conference}} on {{Sustainable Energy Information Technology}}},
  volume = {175},
  pages = {689--694},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.07.101},
  urldate = {2024-04-14},
  abstract = {Automatic emotion recognition based on facial expression is an interesting research field, which has presented and applied in several areas such as safety, health and in human machine interfaces. Researchers in this field are interested in developing techniques to interpret, code facial expressions and extract these features in order to have a better prediction by computer. With the remarkable success of deep learning, the different types of architectures of this technique are exploited to achieve a better performance. The purpose of this paper is to make a study on recent works on automatic facial emotion recognition FER via deep learning. We underline on these contributions treated, the architecture and the databases used and we present the progress made by comparing the proposed methods and the results obtained. The interest of this paper is to serve and guide researchers by review recent works and providing insights to make improvements to this field.},
  keywords = {automatic recognition,database,deep neural networks,facial emotion recognition},
  file = {/Users/aadiwaghray/Zotero/storage/S8R875P3/Mellouk and Handouzi - 2020 - Facial emotion recognition using deep learning re.pdf;/Users/aadiwaghray/Zotero/storage/AL52Y6F3/S1877050920318019.html}
}

@inproceedings{miyakoshiFacialEmotionDetection2011,
  title = {Facial Emotion Detection Considering Partial Occlusion of Face Using {{Bayesian}} Network},
  booktitle = {2011 {{IEEE Symposium}} on {{Computers}} \& {{Informatics}}},
  author = {Miyakoshi, Yoshihiro and Kato, Shohei},
  year = {2011},
  month = mar,
  pages = {96--101},
  doi = {10.1109/ISCI.2011.5958891},
  urldate = {2024-04-14},
  abstract = {Recently, robots that communicate with human have attracted much attention in the research field of robotics. In communication between human, almost all human recognize the subtleties of emotion in each other's facial expressions, voices, and motions. Robots can communicate more smoothly with human as they detect human emotions and respond with appropriate behaviors. Usually, almost all human express their own emotions with their facial expressions. In this paper, we propose an emotion detection system with facial features using a Bayesian network. In actual communication, it is possible that some parts of the face will be occluded by adornments such as glasses or a hat. In previous studies on facial recognition, these studies have been had the process to fill in the gaps of occluded features after capturing facial features from each image. However, not all occluded features can always be filled in the gaps accurately. Therefore, it is difficult for robots to detect emotions accurately in real-time communication. For this reason, we propose an emotion detection system taking into consideration partial occlusion of the face using causal relations between facial features. Bayesian network classifiers infer from the dependencies among the target attribute and explanatory variables. This characteristic of Bayesian network makes our proposed system can detect emotions without filling in the gaps of occluded features. In the experiments, the proposed system succeeded in detecting emotions with high recognition rates even though some facial features were occluded.},
  keywords = {Bayesian methods,Bayesian Network,Emotion recognition,Face,Face recognition,Facial Emotion Detection,Facial features,Feature Selection,Humans,K2 Algorithm,Partial Occlusion of Face},
  file = {/Users/aadiwaghray/Zotero/storage/P6NVZJR7/Miyakoshi and Kato - 2011 - Facial emotion detection considering partial occlu.pdf}
}

@inproceedings{mukhopadhyayFacialEmotionDetection2020,
  title = {Facial {{Emotion Detection}} to {{Assess Learner}}'s {{State}} of {{Mind}} in an {{Online Learning System}}},
  booktitle = {Proceedings of the 2020 5th {{International Conference}} on {{Intelligent Information Technology}}},
  author = {Mukhopadhyay, Moutan and Pal, Saurabh and Nayyar, Anand and Pramanik, Pijush Kanti Dutta and Dasgupta, Niloy and Choudhury, Prasenjit},
  year = {2020},
  month = jun,
  series = {{{ICIIT}} '20},
  pages = {107--115},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3385209.3385231},
  urldate = {2024-04-13},
  abstract = {Despite the success and the popularity of the online learning system, it still lacks in dynamically adapting suitable pedagogical methods according to the changing emotions and behaviour of the learner, as can be done in the face-to-face mode of learning. This makes the learning process mechanized, which significantly affects the learning outcome. To resolve this, the first and necessary step is to assess the emotion of a learner and identify the change of emotions during a learning session. Usually, images of facial expressions are analysed to assess one's state of mind. However, human emotions are far more complex, and these psychological states may not be reflected only through the basic emotion of a learner (i.e. analysing a single image), but a combination of two or more emotions which may be reflected on the face over a period of time. From a real survey, we derived four complex emotions that are a combination of basic human emotions often experienced by a learner, in concert, during a learning session. To capture these combined emotions correctly, we considered a fixed set of continuous image frames, instead of discrete images. We built a CNN model to classify the basic emotions and then identify the states of mind of the learners. The outcome is verified mathematically as well as surveying the learners. The results show a 65\% and 62\% accuracy respectively, for emotion classification and state of mind identification.},
  isbn = {978-1-4503-7659-4},
  keywords = {CNN,Combined emotion,Emotion detection,Facial expression,Image processing,Machine learning,Online learning systems,State of mind}
}

@inproceedings{ngoFacialExpressionRecognition2019,
  title = {Facial {{Expression Recognition}} on {{Static Images}}},
  booktitle = {Future {{Data}} and {{Security Engineering}}},
  author = {Ngo, Tan Quan and Yoon, Seokhoon},
  editor = {Dang, Tran Khanh and K{\"u}ng, Josef and Takizawa, Makoto and Bui, Son Ha},
  year = {2019},
  pages = {640--647},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-35653-8_42},
  abstract = {Facial expression recognition (FER) is currently one of the most attractive and also the most challenging topics in the computer vision and artificial fields. FER applications are ranging from medical treatment, virtual reality, to driver fatigue surveillance, and many other human-machine interaction systems. Benefit from the recent success of deep learning techniques, especially the invention of convolution neural networks (CNN), various end-to-end deep learning-based FER systems have been proposed in the past few years. However, overfitting caused by a lack of training data is still the big challenge that almost all deep FER systems have to put into a concern to achieve high-performance accuracy. In this paper, we are going to build a FER model to recognize eight commons emotions: neutral, happiness, sadness, surprise, fear, disgust, anger, and contempt on the AffectNet dataset. In order to mitigate the effect of small training data, which is prone to overfitting, we proposed a thoughtful transfer learning framework. Specifically, we fine-tuning ResNet-50 model, which is pre-trained on ImageNet dataset for object detection task, on the AffectNet dataset to recognize eight above mentioned face emotions. Experiment results demonstrate the effectiveness of our proposed FER model.},
  isbn = {978-3-030-35653-8},
  langid = {english},
  file = {/Users/aadiwaghray/Zotero/storage/VF8QBBU9/Ngo and Yoon - 2019 - Facial Expression Recognition on Static Images.pdf}
}

@article{ninausIncreasedEmotionalEngagement2019,
  title = {Increased Emotional Engagement in Game-Based Learning \textendash{} {{A}} Machine Learning Approach on Facial Emotion Detection Data},
  author = {Ninaus, Manuel and Greipl, Simon and Kiili, Kristian and Lindstedt, Antero and Huber, Stefan and Klein, Elise and Karnath, Hans-Otto and Moeller, Korbinian},
  year = {2019},
  month = dec,
  journal = {Computers \& Education},
  volume = {142},
  pages = {103641},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2019.103641},
  urldate = {2024-04-14},
  abstract = {It is often argued that game-based learning is particularly effective because of the emotionally engaging nature of games. We employed both automatic facial emotion detection as well as subjective ratings to evaluate emotional engagement of adult participants completing either a game-based numerical task or a non-game-based equivalent. Using a machine learning approach on facial emotion detection data we were able to predict whether individual participants were engaged in the game-based or non-game-based task with classification accuracy significantly above chance level. Moreover, facial emotion detection as well as subjective ratings consistently indicated increased positive as well as negative emotions during game-based learning. These results substantiate that the emotionally engaging nature of games facilitates learning.},
  keywords = {Emotions,Game-based learning,Human-computer interface,Interactive learning environments,Media in education},
  file = {/Users/aadiwaghray/Zotero/storage/BZGGPIAL/S0360131519301940.html}
}

@article{riyantokoFacialEmotionDetection2021a,
  title = {Facial {{Emotion Detection Using Haar-Cascade Classifier}} and {{Convolutional Neural Networks}}},
  author = {Riyantoko, P A and {Sugiarto} and Hindrayani, K M},
  year = {2021},
  month = mar,
  journal = {Journal of Physics: Conference Series},
  volume = {1844},
  number = {1},
  pages = {012004},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1844/1/012004},
  urldate = {2024-04-14},
  abstract = {Computer vision has the challenge to detect the facial emotions of humans. Recently, in computer vision and machine learning, it's possible to detect emotion from video or image accurate. In our research will propose to classify facial emotion using Haar-Cascade Classifier and Convolutional Neural Networks. The experiment uses the FER2013 dataset which was collected for the facial expression recognition dataset, and we proposed seven classified facial expression. The CNN model gain MSE and accuracy value based on epoch variety. The results showed that with the increase in the epoch value, the smaller MSE value would be obtained, likewise, the accuracy value would be increased. Thus, the proposed algorithm of CNN is proven to be effective for facial emotion detection.},
  langid = {english},
  file = {/Users/aadiwaghray/Zotero/storage/A3ILT3LJ/Riyantoko et al. - 2021 - Facial Emotion Detection Using Haar-Cascade Classi.pdf}
}

@inproceedings{ruiz-garciaGenerativeAdversarialStacked2020,
  title = {Generative {{Adversarial Stacked Autoencoders}} for {{Facial Pose Normalization}} and {{Emotion Recognition}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{Ruiz-Garcia}, Ariel and Palade, Vasile and Elshaw, Mark and Awad, Mariette},
  year = {2020},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN48605.2020.9207170},
  urldate = {2024-04-15},
  abstract = {In this work, we propose a novel Generative Adversarial Stacked Autoencoder that learns to map facial expressions with up to {$\pm$}60 degrees to an illumination invariant facial representation of 0 degrees. We accomplish this by using a novel convolutional layer that exploits both local and global spatial information, and a convolutional layer with a reduced number of parameters that exploits facial symmetry. Furthermore, we introduce a generative adversarial gradual greedy layer-wise learning algorithm designed to train Adversarial Autoencoders in an efficient and incremental manner. We demonstrate the efficiency of our method and report state-of-the-art performance on several facial emotion recognition corpora, including one collected in the wild.},
  keywords = {Emotion recognition,Emotion Recognition,Face recognition,Facial Pose Normalization,Gallium nitride,Generative Adversarial Networks,Generative Adversarial Stacked Autoencoders,Illumination Invariance,Image reconstruction,Kernel,Lighting,Training},
  file = {/Users/aadiwaghray/Zotero/storage/4QSZSNL4/Ruiz-Garcia et al. - 2020 - Generative Adversarial Stacked Autoencoders for Fa.pdf}
}

@inproceedings{ruiz-garciaStackedDeepConvolutional2017,
  title = {Stacked Deep Convolutional Auto-Encoders for Emotion Recognition from Facial Expressions},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{Ruiz-Garcia}, Ariel and Elshaw, Mark and Altahhan, Abdulrahman and Palade, Vasile},
  year = {2017},
  month = may,
  pages = {1586--1593},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966040},
  urldate = {2024-04-15},
  abstract = {Emotion recognition is critical for everyday living and is essential for meaningful interaction. If we are to progress towards human and machine interaction that is engaging the human user, the machine should be able to recognize the emotional state of the user. Deep Convolutional Neural Networks (CNN) have proven to be efficient in emotion recognition problems. The good degree of performance achieved by these classifiers can be attributed to their ability to self-learn a down-sampled feature vector that retains spatial information through filter kernels in convolutional layers. Given the view that random initialization of weights can lead to convergence to non-optimal local minima, in this paper we explore the impact of training the initial weights in an unsupervised manner. We study the effect of pre-training a Deep CNN as a Stacked Convolutional Auto-Encoder (SCAE) in a greedy layer-wise unsupervised fashion for emotion recognition using facial expression images. When trained with randomly initialized weights, our CNN emotion recognition model achieves a performance rate of 91.16\% on the Karolinska Directed Emotional Faces (KDEF) dataset. In contrast, when each layer of the model, including the hidden layer, is pre-trained as an Auto-Encoder, the performance increases to 92.52\%. Pre-training our CNN as a SCAE also reduces training time marginally. The emotion recognition model developed in this work will form the basis of a real-time empathic robot system.},
  keywords = {Convergence,Convolution,Emotion recognition,Feature extraction,Kernel,Robots,Training},
  file = {/Users/aadiwaghray/Zotero/storage/7B7VF4KX/Ruiz-Garcia et al. - 2017 - Stacked deep convolutional auto-encoders for emoti.pdf}
}

@article{sauterCrossculturalRecognitionBasic2010,
  title = {Cross-Cultural Recognition of Basic Emotions through Nonverbal Emotional Vocalizations},
  author = {Sauter, Disa A. and Eisner, Frank and Ekman, Paul and Scott, Sophie K.},
  year = {2010},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {6},
  pages = {2408--2412},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.0908239106},
  urldate = {2024-04-16},
  abstract = {Emotional signals are crucial for sharing important information, with conspecifics, for example, to warn humans of danger. Humans use a range of different cues to communicate to others how they feel, including facial, vocal, and gestural signals. We examined the recognition of nonverbal emotional vocalizations, such as screams and laughs, across two dramatically different cultural groups. Western participants were compared to individuals from remote, culturally isolated Namibian villages. Vocalizations communicating the so-called ``basic emotions'' (anger, disgust, fear, joy, sadness, and surprise) were bidirectionally recognized. In contrast, a set of additional emotions was only recognized within, but not across, cultural boundaries. Our findings indicate that a number of primarily negative emotions have vocalizations that can be recognized across cultures, while most positive emotions are communicated with culture-specific signals.},
  file = {/Users/aadiwaghray/Zotero/storage/98B3AE45/Sauter et al. - 2010 - Cross-cultural recognition of basic emotions throu.pdf}
}

@article{saxena1EmotionRecognitionDetection2020,
  title = {Emotion {{Recognition}} and {{Detection Methods}}: {{A Comprehensive Survey}}},
  shorttitle = {Emotion {{Recognition}} and {{Detection Methods}}},
  author = {Saxena\textsuperscript{1}, {$<$}p{$>$}Anvita and Khanna\textsuperscript{1}, Ashish and Gupta\textsuperscript{1}, Deepak and *{$<$}/p{$>$}},
  year = {2020},
  month = feb,
  journal = {Journal of Artificial Intelligence and Systems},
  volume = {2},
  number = {1},
  pages = {53--79},
  publisher = {{Institute of Electronics and Computer}},
  issn = {2642-2859},
  doi = {10.33969/AIS.2020.21005},
  urldate = {2024-04-14},
  abstract = {{$<$}p{$>$}Human emotion recognition through artificial intelligence is one of the most popular research fields among researchers nowadays. The fields of Human Computer Interaction (HCI) and Affective Computing are being extensively used to sense human emotions. Humans generally use a lot of indirect and non-verbal means to convey their emotions. The presented exposition aims to provide an overall overview with the analysis of all the noteworthy emotion detection methods at a single location. To the best of our knowledge, this is the first attempt to outline all the emotion recognition models developed in the last decade. The paper is comprehended by expending more than hundred papers; a detailed analysis of the methodologies along with the datasets is carried out in the paper. The study revealed that emotion detection is predominantly carried out through four major methods, namely, facial expression recognition, physiological signals recognition, speech signals variation and text semantics on standard databases such as JAFFE, CK+, Berlin Emotional Database, SAVEE, etc. as well as self-generated databases. Generally seven basic emotions are recognized through these methods. Further, we have compared different methods employed for emotion detection in humans. The best results were obtained by using Stationary Wavelet Transform for Facial Emotion Recognition , Particle Swarm Optimization assisted Biogeography based optimization algorithms for emotion recognition through speech, Statistical features coupled with different methods for physiological signals, Rough set theory coupled with SVM for text semantics with respective accuracies of 98.83\%,99.47\%, 87.15\%,87.02\% . Overall, the method of Particle Swarm Optimization assisted Biogeography based optimization algorithms with an accuracy of 99.47\% on BES dataset gave the best results.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/aadiwaghray/Zotero/storage/I6F7N9GI/Saxena1 et al. - 2020 - Emotion Recognition and Detection Methods A Compr.pdf}
}

@article{schofieldSocialAnxietyInterpretation2007,
  title = {Social Anxiety and Interpretation Biases for Facial Displays of Emotion: {{Emotion}} Detection and Ratings of Social Cost},
  shorttitle = {Social Anxiety and Interpretation Biases for Facial Displays of Emotion},
  author = {Schofield, Casey A. and Coles, Meredith E. and Gibb, Brandon E.},
  year = {2007},
  month = dec,
  journal = {Behaviour Research and Therapy},
  volume = {45},
  number = {12},
  pages = {2950--2963},
  issn = {0005-7967},
  doi = {10.1016/j.brat.2007.08.006},
  urldate = {2024-04-14},
  abstract = {The current study assessed the processing of facial displays of emotion (Happy, Disgust, and Neutral) of varying emotional intensities in participants with high vs. low social anxiety. Use of facial expressions of varying intensities allowed for strong external validity and a fine-grained analysis of interpretation biases. Sensitivity to perceiving negative evaluation in faces (i.e., emotion detection) was assessed at both long (unlimited) and brief (60ms) stimulus durations. In addition, ratings of perceived social cost were made indicating what participants judged it would be like to have a social interaction with a person exhibiting the stimulus emotion. Results suggest that high social anxiety participants did not demonstrate biases in their sensitivity to perceiving negative evaluation (i.e. disgust) in facial expressions. However, high social anxiety participants did estimate the perceived cost of interacting with someone showing disgust to be significantly greater than low social anxiety participants, regardless of the intensity of the disgust expression. These results are consistent with a specific type of interpretation bias in which participants with social anxiety have elevated ratings of the social cost of interacting with individuals displaying negative evaluation.},
  keywords = {Bias,Disgust,Emotion,Faces,Happy,Interpretation,Social anxiety,Social phobia},
  file = {/Users/aadiwaghray/Zotero/storage/UTCUJQP5/S0005796707001702.html}
}

@article{smithaFacialEmotionRecognition2015,
  title = {Facial Emotion Recognition System for Autistic Children: A Feasible Study Based on {{FPGA}} Implementation},
  shorttitle = {Facial Emotion Recognition System for Autistic Children},
  author = {Smitha, K. G. and Vinod, A. P.},
  year = {2015},
  month = nov,
  journal = {Medical \& Biological Engineering \& Computing},
  volume = {53},
  number = {11},
  pages = {1221--1229},
  issn = {1741-0444},
  doi = {10.1007/s11517-015-1346-z},
  urldate = {2024-04-14},
  abstract = {Children with autism spectrum disorder have difficulty in understanding the emotional and mental states from the facial expressions of the people they interact. The inability to understand other people's emotions will hinder their interpersonal communication. Though many facial emotion recognition algorithms have been proposed in the literature, they are mainly intended for processing by a personal computer, which limits their usability in on-the-move applications where portability is desired. The portability of the system will ensure ease of use and real-time emotion recognition and that will aid for immediate feedback while communicating with caretakers. Principal component analysis (PCA) has been identified as the least complex feature extraction algorithm to be implemented in hardware. In this paper, we present a detailed study of the implementation of serial and parallel implementation of PCA in order to identify the most feasible method for realization of a portable emotion detector for autistic children. The proposed emotion recognizer architectures are implemented on Virtex 7 XC7VX330T FFG1761-3 FPGA. We achieved 82.3~\% detection accuracy for a word length of 8 bits.},
  langid = {english},
  keywords = {Facial emotion recognition,FPGA implementation,Real-time and portability},
  file = {/Users/aadiwaghray/Zotero/storage/IQS7LWJM/Smitha and Vinod - 2015 - Facial emotion recognition system for autistic chi.pdf}
}

@article{xieDeepMultipathConvolutional2019,
  title = {Deep Multi-Path Convolutional Neural Network Joint with Salient Region Attention for Facial Expression Recognition},
  author = {Xie, Siyue and Hu, Haifeng and Wu, Yongbo},
  year = {2019},
  month = aug,
  journal = {Pattern Recognition},
  volume = {92},
  pages = {177--191},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2019.03.019},
  urldate = {2024-04-15},
  abstract = {Facial Expression Recognition (FER) has long been a challenging task in the field of computer vision. In this paper, we present a novel model, named Deep Attentive Multi-path Convolutional Neural Network (DAM-CNN), for FER. Different from most existing models, DAM-CNN can automatically locate expression-related regions in an expressional image and yield a robust image representation for FER. The proposed model contains two novel modules: an attention-based Salient Expressional Region Descriptor (SERD) and the Multi-Path Variation-Suppressing Network (MPVS-Net). SERD can adaptively estimate the importance of different image regions for FER task, while MPVS-Net disentangles expressional information from irrelevant variations. By jointly combining SERD and MPVS-Net, DAM-CNN is able to highlight expression-relevant features and generate a variation-robust representation for expression classification. Extensive experimental results on both constrained datasets (CK+, JAFFE, TFEID) and unconstrained datasets (SFEW, FER2013, BAUM-2i) demonstrate the effectiveness of our DAM-CNN model.},
  keywords = {Attention,Convolutional neural network,Facial expression recognition,Multi-Path variation-suppressing network,Salient expressional region descriptor},
  file = {/Users/aadiwaghray/Zotero/storage/PAJWFEMW/Xie et al. - 2019 - Deep multi-path convolutional neural network joint.pdf;/Users/aadiwaghray/Zotero/storage/E2BAENEV/S0031320319301268.html}
}

@article{yangBenchmarkingCommercialEmotion2021a,
  title = {Benchmarking Commercial Emotion Detection Systems Using Realistic Distortions of Facial Image Datasets},
  author = {Yang, Kangning and Wang, Chaofan and Sarsenbayeva, Zhanna and Tag, Benjamin and Dingler, Tilman and Wadley, Greg and Goncalves, Jorge},
  year = {2021},
  month = jun,
  journal = {The Visual Computer},
  volume = {37},
  number = {6},
  pages = {1447--1466},
  issn = {0178-2789, 1432-2315},
  doi = {10.1007/s00371-020-01881-x},
  urldate = {2024-04-14},
  abstract = {Currently, there are several widely used commercial cloud-based services that attempt to recognize an individual's emotions based on their facial expressions. Most research into facial emotion recognition has used high-resolution, front-oriented, full-face images. However, when images are collected in naturalistic settings (e.g., using smartphone's frontal camera), these images are likely to be far from ideal due to camera positioning, lighting conditions, and camera shake. The impact these conditions have on the accuracy of commercial emotion recognition services has not been studied in full detail. To fill this gap, we selected five prominent commercial emotion recognition systems\textemdash Amazon Rekognition, Baidu Research, Face++, Microsoft Azure, and Affectiva\textemdash and evaluated their performance via two experiments. In Experiment 1, we compared the systems' accuracy at classifying images drawn from three standardized facial expression databases. In Experiment 2, we first identified several common scenarios (e.g., partially visible face) that can lead to poor-quality pictures during smartphone use, and manipulated the same set of images used in Experiment 1 to simulate these scenarios. We used the manipulated images to again compare the systems' classification performance, finding that the systems varied in how well they handled manipulated images that simulate realistic image distortion. Based on our findings, we offer recommendations for developers and researchers who would like to use commercial facial emotion recognition technologies in their applications.},
  langid = {english},
  file = {/Users/aadiwaghray/Zotero/storage/WE6M4IRH/Yang et al. - 2021 - Benchmarking commercial emotion detection systems .pdf}
}

@article{zengFacialExpressionRecognition2018,
  title = {Facial Expression Recognition via Learning Deep Sparse Autoencoders},
  author = {Zeng, Nianyin and Zhang, Hong and Song, Baoye and Liu, Weibo and Li, Yurong and Dobaie, Abdullah M.},
  year = {2018},
  month = jan,
  journal = {Neurocomputing},
  volume = {273},
  pages = {643--649},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.08.043},
  urldate = {2024-04-15},
  abstract = {Facial expression recognition is an important research issue in the pattern recognition field. In this paper, we intend to present a novel framework for facial expression recognition to automatically distinguish the expressions with high accuracy. Especially, a high-dimensional feature composed by the combination of the facial geometric and appearance features is introduced to the facial expression recognition due to its containing the accurate and comprehensive information of emotions. Furthermore, the deep sparse autoencoders (DSAE) are established to recognize the facial expressions with high accuracy by learning robust and discriminative features from the data. The experiment results indicate that the presented framework can achieve a high recognition accuracy of 95.79\% on the extended Cohn\textendash Kanade (CK+) database for seven facial expressions, which outperforms the other three state-of-the-art methods by as much as 3.17\%, 4.09\% and 7.41\%, respectively. In particular, the presented approach is also applied to recognize eight facial expressions (including the neutral) and it provides a satisfactory recognition accuracy, which successfully demonstrates the feasibility and effectiveness of the approach in this paper.},
  keywords = {Deep architecture,Facial expression recognition,High-dimensional feature,Histogram of oriented gradients (HOG),Sparse autoencoders},
  file = {/Users/aadiwaghray/Zotero/storage/82J83M2N/Zeng et al. - 2018 - Facial expression recognition via learning deep sp.pdf;/Users/aadiwaghray/Zotero/storage/7DDWQ4D3/S0925231217314649.html}
}
